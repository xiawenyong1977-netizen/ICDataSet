import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
import os
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def generate_marble_countertop():
    """
    ä½¿ç”¨æŒ‡å®šæç¤ºè¯ç”Ÿæˆå¤§ç†çŸ³å°é¢èƒŒæ™¯å›¾ç‰‡
    """
    print("ğŸ–¥ï¸  å¼€å§‹ç”Ÿæˆå¤§ç†çŸ³å°é¢èƒŒæ™¯å›¾ç‰‡...")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "desktop_backgrounds"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # åŠ è½½æ¨¡å‹
        print("ğŸ“¥ æ­£åœ¨åŠ è½½Stable Diffusionæ¨¡å‹...")
        model_id = "runwayml/stable-diffusion-v1-5"
        
        scheduler = DPMSolverMultistepScheduler.from_pretrained(
            model_id, 
            subfolder="scheduler"
        )
        
        pipeline = StableDiffusionPipeline.from_pretrained(
            model_id,
            scheduler=scheduler,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        if device == "cuda":
            pipeline = pipeline.to(device)
            pipeline.enable_attention_slicing()
            pipeline.enable_vae_slicing()
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # ä½¿ç”¨æŒ‡å®šçš„æç¤ºè¯
        prompt = "Soft linen bedsheet, neatly made, subtle folds, natural window light, shallow depth of field, professional product photography"
        negative_prompt = "extra digit, fewer digits, cropped, worst quality, low quality"
        
        print(f"\nğŸ¨ æ­£åœ¨ç”Ÿæˆ: å¤§ç†çŸ³å°é¢")
        print(f"æ­£é¢æç¤ºè¯: {prompt}")
        print(f"è´Ÿé¢æç¤ºè¯: {negative_prompt}")
        
        try:
            # ç”Ÿæˆå›¾ç‰‡
            image = pipeline(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=1024,
                height=768,
                num_inference_steps=20,
                guidance_scale=7.5,
                num_images_per_prompt=1
            ).images[0]
            
            # ä¿å­˜å›¾ç‰‡
            filename = "å¤§ç†çŸ³å°é¢_linen_bedsheet.png"
            output_path = os.path.join(output_dir, filename)
            image.save(output_path)
            
            print(f"âœ… ç”ŸæˆæˆåŠŸ: {filename}")
            print(f"ğŸ“ ä¿å­˜è·¯å¾„: {output_path}")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return False
        
        print(f"\nğŸ‰ å¤§ç†çŸ³å°é¢èƒŒæ™¯ç”Ÿæˆå®Œæˆï¼")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        print("\nğŸ’¡ è¿™ä¸ªèƒŒæ™¯å›¾çš„ç‰¹ç‚¹:")
        print("1. æŸ”è½¯çš„äºšéº»åºŠå•çº¹ç†")
        print("2. æ•´é½çš„æŠ˜å æ•ˆæœ")
        print("3. è‡ªç„¶çª—æˆ·å…‰çº¿")
        print("4. æµ…æ™¯æ·±æ•ˆæœ")
        print("5. ä¸“ä¸šäº§å“æ‘„å½±é£æ ¼")
        print("6. å®Œç¾é€‚åˆæ”¾ç½®èº«ä»½è¯")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        return False

if __name__ == "__main__":
    generate_marble_countertop()
